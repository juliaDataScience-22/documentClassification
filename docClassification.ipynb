{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnxJpyW3i-_k"
      },
      "source": [
        "# Document Classification\n",
        "### Team The p < 0.05 Team - Haig Bedros, Noori Selina, Julia Ferris, Matthew Roland\n",
        "\n",
        "\n",
        "It can be useful to classify new \"test\" documents using already classified \"training\" documents. A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam. Here is one example of such data: UCI Machine Learning Repository: Spambase Data Set.\n",
        "\n",
        "For this project, we used the BBC Full Text Document Classification dataset from Kaggle. This dataset contains full documents categorized into five categories: business, entertainment, politics, sport, and tech. \n",
        "\n",
        "The goal of our text classification is to predict the category of new documents in the test set.\n",
        "\n",
        "Link to the dataset: https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification?resource=download\n",
        "\n",
        "The models we used include the Naive Bayes Classifier, Support Vector Machines, and Random Forests. The results were compared for accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU2SDyuXmbHZ",
        "outputId": "3a37be44-b79e-45e0-d8fa-a1df33711914"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to get all .txt dat files from subfolders\n",
        "def get_txt_files_from_github(category_url):\n",
        "    response = requests.get(category_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    files = []\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        if href and href.endswith('.txt'):\n",
        "            files.append(href.split('/')[-1])\n",
        "    return files\n",
        "\n",
        "# Github repo\n",
        "base_url = \"https://github.com/juliaDataScience-22/documentClassification/tree/main/data/bbc\"\n",
        "raw_base_url = \"https://raw.githubusercontent.com/juliaDataScience-22/documentClassification/main/data/bbc\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6AbGJ1iUgqc"
      },
      "source": [
        "## 1. Load and Process the Documents\n",
        "\n",
        "The zip file of the dataset is extracted, and the documents from different categories are loaded and processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_z7ckYOUkFGR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents: 800\n",
            "Sample document: (['Ad', 'sales', 'boost', 'Time', 'Warner', 'profit', 'Quarterly', 'profits', 'at', 'US', 'media', 'giant', 'TimeWarner', 'jumped', '76%', 'to', '$1.13bn', '(Â£600m)', 'for', 'the', 'three', 'months', 'to', 'December,', 'from', '$639m', 'year-earlier.', 'The', 'firm,', 'which', 'is', 'now', 'one', 'of', 'the', 'biggest', 'investors', 'in', 'Google,', 'benefited', 'from', 'sales', 'of', 'high-speed', 'internet', 'connections', 'and', 'higher', 'advert', 'sales.', 'TimeWarner', 'said', 'fourth', 'quarter', 'sales', 'rose', '2%', 'to', '$11.1bn', 'from', '$10.9bn.', 'Its', 'profits', 'were', 'buoyed', 'by', 'one-off', 'gains', 'which', 'offset', 'a', 'profit', 'dip', 'at', 'Warner', 'Bros,', 'and', 'less', 'users', 'for', 'AOL.', 'Time', 'Warner', 'said', 'on', 'Friday', 'that', 'it', 'now', 'owns', '8%', 'of', 'search-engine', 'Google.', 'But', 'its', 'own', 'internet', 'business,', 'AOL,', 'had', 'has', 'mixed', 'fortunes.', 'It', 'lost', '464,000', 'subscribers', 'in', 'the', 'fourth', 'quarter', 'profits', 'were', 'lower', 'than', 'in', 'the', 'preceding', 'three', 'quarters.', 'However,', 'the', 'company', 'said', \"AOL's\", 'underlying', 'profit', 'before', 'exceptional', 'items', 'rose', '8%', 'on', 'the', 'back', 'of', 'stronger', 'internet', 'advertising', 'revenues.', 'It', 'hopes', 'to', 'increase', 'subscribers', 'by', 'offering', 'the', 'online', 'service', 'free', 'to', 'TimeWarner', 'internet', 'customers', 'and', 'will', 'try', 'to', 'sign', 'up', \"AOL's\", 'existing', 'customers', 'for', 'high-speed', 'broadband.', 'TimeWarner', 'also', 'has', 'to', 'restate', '2000', 'and', '2003', 'results', 'following', 'a', 'probe', 'by', 'the', 'US', 'Securities', 'Exchange', 'Commission', '(SEC),', 'which', 'is', 'close', 'to', 'concluding.', 'Time', \"Warner's\", 'fourth', 'quarter', 'profits', 'were', 'slightly', 'better', 'than', \"analysts'\", 'expectations.', 'But', 'its', 'film', 'division', 'saw', 'profits', 'slump', '27%', 'to', '$284m,', 'helped', 'by', 'box-office', 'flops', 'Alexander', 'and', 'Catwoman,', 'a', 'sharp', 'contrast', 'to', 'year-earlier,', 'when', 'the', 'third', 'and', 'final', 'film', 'in', 'the', 'Lord', 'of', 'the', 'Rings', 'trilogy', 'boosted', 'results.', 'For', 'the', 'full-year,', 'TimeWarner', 'posted', 'a', 'profit', 'of', '$3.36bn,', 'up', '27%', 'from', 'its', '2003', 'performance,', 'while', 'revenues', 'grew', '6.4%', 'to', '$42.09bn.', '\"Our', 'financial', 'performance', 'was', 'strong,', 'meeting', 'or', 'exceeding', 'all', 'of', 'our', 'full-year', 'objectives', 'and', 'greatly', 'enhancing', 'our', 'flexibility,\"', 'chairman', 'and', 'chief', 'executive', 'Richard', 'Parsons', 'said.', 'For', '2005,', 'TimeWarner', 'is', 'projecting', 'operating', 'earnings', 'growth', 'of', 'around', '5%,', 'and', 'also', 'expects', 'higher', 'revenue', 'and', 'wider', 'profit', 'margins.', 'TimeWarner', 'is', 'to', 'restate', 'its', 'accounts', 'as', 'part', 'of', 'efforts', 'to', 'resolve', 'an', 'inquiry', 'into', 'AOL', 'by', 'US', 'market', 'regulators.', 'It', 'has', 'already', 'offered', 'to', 'pay', '$300m', 'to', 'settle', 'charges,', 'in', 'a', 'deal', 'that', 'is', 'under', 'review', 'by', 'the', 'SEC.', 'The', 'company', 'said', 'it', 'was', 'unable', 'to', 'estimate', 'the', 'amount', 'it', 'needed', 'to', 'set', 'aside', 'for', 'legal', 'reserves,', 'which', 'it', 'previously', 'set', 'at', '$500m.', 'It', 'intends', 'to', 'adjust', 'the', 'way', 'it', 'accounts', 'for', 'a', 'deal', 'with', 'German', 'music', 'publisher', \"Bertelsmann's\", 'purchase', 'of', 'a', 'stake', 'in', 'AOL', 'Europe,', 'which', 'it', 'had', 'reported', 'as', 'advertising', 'revenue.', 'It', 'will', 'now', 'book', 'the', 'sale', 'of', 'its', 'stake', 'in', 'AOL', 'Europe', 'as', 'a', 'loss', 'on', 'the', 'value', 'of', 'that', 'stake.'], 'business')\n",
            "Total words: 265532\n"
          ]
        }
      ],
      "source": [
        "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "documents = []\n",
        "all_words_list = []\n",
        "\n",
        "for category in categories:\n",
        "    category_url = f\"{base_url}/{category}\"\n",
        "    txt_files = get_txt_files_from_github(category_url)\n",
        "    \n",
        "    for filename in txt_files:\n",
        "        file_url = f\"{raw_base_url}/{category}/{filename}\"\n",
        "        try:\n",
        "            file_response = requests.get(file_url)\n",
        "            file_response.encoding = 'utf-8'\n",
        "            words = file_response.text.split()\n",
        "        except UnicodeDecodeError:\n",
        "            file_response.encoding = 'ISO-8859-1'\n",
        "            words = file_response.text.split()\n",
        "        documents.append((words, category))\n",
        "        all_words_list.extend(w.lower() for w in words)\n",
        "\n",
        "# Print some data to verify\n",
        "print(f\"Total documents: {len(documents)}\")\n",
        "print(f\"Sample document: {documents[0]}\")\n",
        "print(f\"Total words: {len(all_words_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKuIOQnDQaiq"
      },
      "source": [
        "## 2. Feature Extraction\n",
        "Extract features using NLTK for the Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "luNxc3mXHpY4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk import FreqDist\n",
        "\n",
        "all_words = FreqDist(w.lower() for w in all_words_list)\n",
        "word_features = list(all_words)[:1000]\n",
        "\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
        "\n",
        "random.shuffle(featuresets)\n",
        "total_samples = len(featuresets)\n",
        "train_size = int(0.7 * total_samples)\n",
        "train_set, test_set = featuresets[:train_size], featuresets[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-lHoMoORPF0"
      },
      "source": [
        "## 3. Training and Evaluation of Naive Bayes Classifier\n",
        "Train and evaluate the NLTK Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvkUS6tsIigY",
        "outputId": "3f83c4f2-1865-49b6-d9ff-30d167a3a513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Naive Bayes Accuracy: 0.9541666666666667\n",
            "Most Informative Features\n",
            "          contains(race) = True            sport : busine =     37.7 : 1.0\n",
            "         contains(actor) = True           entert : politi =     37.0 : 1.0\n",
            "          contains(film) = True           entert : politi =     29.6 : 1.0\n",
            "        contains(leader) = True           politi : sport  =     28.5 : 1.0\n",
            "          contains(star) = True           entert : politi =     27.7 : 1.0\n",
            "         contains(award) = True           entert : busine =     27.1 : 1.0\n",
            "      contains(starring) = True           entert : politi =     26.9 : 1.0\n",
            "         contains(drugs) = True            sport : busine =     24.3 : 1.0\n",
            "           contains(won) = True            sport : busine =     23.1 : 1.0\n",
            "         contains(party) = True           politi : entert =     22.5 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print('NLTK Naive Bayes Accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
        "classifier.show_most_informative_features(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MlsChmQVtuj"
      },
      "source": [
        "**Naive Bayes Classifier Results:**\n",
        "- **Accuracy**: 92.3%\n",
        "- **Summary**: The NLTK Naive Bayes classifier achieved an accuracy of 92.3%, meaning it correctly classified the documents into their respective categories (business, entertainment, politics, sport, and tech) 92.3% of the time. The most informative features were words like 'said', 'race', 'won', 'growth', and 'gold', which had the highest impact on the classification decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-obqjqjBRVIN"
      },
      "source": [
        "## 4. Prepare Data for SVM Classifier using TF-IDF\n",
        "Convert the documents to TF-IDF features for use with the SVM classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "acSJv0EiIm2r"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def document_to_string(document):\n",
        "    return ' '.join(document)\n",
        "\n",
        "documents_str = [document_to_string(doc) for doc, _ in documents]\n",
        "labels = [label for _, label in documents]\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X = vectorizer.fit_transform(documents_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JL4AFisJiw6"
      },
      "source": [
        "\n",
        "Split the data into training (70%) and testing (30%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "64cGpKkEJkc9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVq_ZBbORsY4"
      },
      "source": [
        "## 5. Train and Evaluate the SVM Classifier\n",
        "Train and evaluate the SVM classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDtdUQCrRu-h",
        "outputId": "e6695bc7-d770-4b8e-8f92-ec122b5f53b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Accuracy: 1.0\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       1.00      1.00      1.00        62\n",
            "entertainment       1.00      1.00      1.00        63\n",
            "     politics       1.00      1.00      1.00        51\n",
            "        sport       1.00      1.00      1.00        64\n",
            "\n",
            "     accuracy                           1.00       240\n",
            "    macro avg       1.00      1.00      1.00       240\n",
            " weighted avg       1.00      1.00      1.00       240\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "svm_classifier = LinearSVC()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "svm_predictions = svm_classifier.predict(X_test)\n",
        "\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "print(f'SVM Accuracy: {svm_accuracy}')\n",
        "print(classification_report(y_test, svm_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3jNKaGwWsrU"
      },
      "source": [
        "**SVM Classifier Results:**\n",
        "- **Accuracy**: 97.0%\n",
        "- **Summary**: The SVM classifier achieved an accuracy of 97.0%, meaning it correctly classified the documents almost all of the time. Each category (business, entertainment, politics, sport, and tech) was classified with very high accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UVcl80_R3-I"
      },
      "source": [
        "## 6. Train and Evaluate the Random Forest Classifier\n",
        "Train and Evaluate the Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3wYSBcwR2cx",
        "outputId": "c909b7e9-691d-422b-9caa-52c5e19fd37e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.9916666666666667\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       1.00      0.97      0.98        62\n",
            "entertainment       1.00      1.00      1.00        63\n",
            "     politics       0.96      1.00      0.98        51\n",
            "        sport       1.00      1.00      1.00        64\n",
            "\n",
            "     accuracy                           0.99       240\n",
            "    macro avg       0.99      0.99      0.99       240\n",
            " weighted avg       0.99      0.99      0.99       240\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(f'Random Forest Accuracy: {rf_accuracy}')\n",
        "print(classification_report(y_test, rf_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvb5w6a2XNNO"
      },
      "source": [
        "**Random Forest Classifier Results:**\n",
        "- **Accuracy**: 94.8%\n",
        "- **Summary**: The Random Forest classifier achieved an accuracy of 94.8%, meaning it correctly classified the documents most of the time. Each category (business, entertainment, politics, sport, and tech) was classified with high precision and recall, indicating the classifier's strong performance across all categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHJwGGgqXPg9"
      },
      "source": [
        "## 7. Conclusion\n",
        "- **Naive Bayes Classifier**:\n",
        "  - **Accuracy**: 89.1%\n",
        "  - **Summary**: The Naive Bayes classifier correctly classified most documents and identified key words for each category.\n",
        "\n",
        "- **SVM Classifier**:\n",
        "  - **Accuracy**: 97.0%\n",
        "  - **Summary**: The SVM classifier was the most accurate, effectively classifying documents with very high precision and recall.\n",
        "\n",
        "- **Random Forest Classifier**:\n",
        "  - **Accuracy**: 94.8%\n",
        "  - **Summary**: The Random Forest classifier also performed well, correctly classifying a large majority of documents.\n",
        "\n",
        "- **Key Outcome**:\n",
        "  - The SVM classifier was the best model for classifying documents into business, entertainment, politics, sport, and tech categories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNZcU6mUi-_t"
      },
      "source": [
        "Citation:\n",
        "\n",
        "D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
